{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f779f6c1e6649588e6aafcf9b575c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84e4aca209fb4a388e3b04d248c2725b",
              "IPY_MODEL_c806c22303814b98b20eeac8703da0e2",
              "IPY_MODEL_9c3f1c48b9ee4bfcb8a7321bc582bf08"
            ],
            "layout": "IPY_MODEL_62a8596a78ac4504b988efa75df1012d"
          }
        },
        "84e4aca209fb4a388e3b04d248c2725b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9a54173688943be8c91005a8959091e",
            "placeholder": "​",
            "style": "IPY_MODEL_77b72eab7ce8444a80b1b9d16795e7e2",
            "value": "Generating Fluorescence:  33%"
          }
        },
        "c806c22303814b98b20eeac8703da0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_064b329f1abb4f5880731072bf2439e1",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc234c902a0c40ffa2117ef36c43e598",
            "value": 1
          }
        },
        "9c3f1c48b9ee4bfcb8a7321bc582bf08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20359d92e174958b52103bf75f4f8f9",
            "placeholder": "​",
            "style": "IPY_MODEL_a0b37a6a66c64c9b8dca00643907bd3b",
            "value": " 1/3 [12:30&lt;25:00, 750.36s/it]"
          }
        },
        "62a8596a78ac4504b988efa75df1012d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a54173688943be8c91005a8959091e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b72eab7ce8444a80b1b9d16795e7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "064b329f1abb4f5880731072bf2439e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc234c902a0c40ffa2117ef36c43e598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c20359d92e174958b52103bf75f4f8f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b37a6a66c64c9b8dca00643907bd3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Training + Inference**"
      ],
      "metadata": {
        "id": "_JkY8e5_6bFN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlybCv7858jK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Conditional Diffusion (DDPM) for Brightfield -> Red/Green\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "# === INSTALL / SETUP ===\n",
        "try:\n",
        "    import diffusers\n",
        "    import torchvision\n",
        "    import accelerate\n",
        "except ImportError:\n",
        "    print(\"Installing required packages...\")\n",
        "    !pip install -q diffusers transformers accelerate safetensors torchvision scikit-image tqdm\n",
        "\n",
        "# === IMPORTS ===\n",
        "import os, json, random\n",
        "from glob import glob\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models # For VGG\n",
        "\n",
        "from diffusers import UNet2DModel, DDPMScheduler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR # New Scheduler\n",
        "\n",
        "# === PATHS ===\n",
        "# Output directory setup\n",
        "BASE = '/content/drive/MyDrive/mayank/Brightfield vs Fluorescent Staining Dataset'\n",
        "OUT_DIR = '/content/drive/MyDrive/mayank/conditional_diffusion_outputs_v4'\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR, 'samples'), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR, 'models'), exist_ok=True)\n",
        "\n",
        "# === 0) PERCEPTUAL LOSS SETUP (VGG) ===\n",
        "class PerceptualLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
        "        self.slice = torch.nn.Sequential()\n",
        "        # Use first 9 layers of VGG for perceptual similarity\n",
        "        for x in range(9):\n",
        "            self.slice.add_module(str(x), vgg[x])\n",
        "        for param in self.slice.parameters():\n",
        "            param.requires_grad = False  # Freeze VGG\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Convert 1-channel image to 3-channel for VGG\n",
        "        pred_3ch = pred.repeat(1, 3, 1, 1)\n",
        "        target_3ch = target.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Bring to VGG range [0,1]\n",
        "        pred_norm = (pred_3ch + 1) / 2\n",
        "        target_norm = (target_3ch + 1) / 2\n",
        "\n",
        "        # Compute L1 between VGG feature maps\n",
        "        return F.l1_loss(self.slice(pred_norm), self.slice(target_norm))\n",
        "\n",
        "# === 1) LIST SUBFOLDERS & SPLIT ===\n",
        "# Auto-discover dataset folders and split into train/val/test\n",
        "all_subfolders = sorted([d for d in os.listdir(BASE) if os.path.isdir(os.path.join(BASE, d))])\n",
        "test_folders = [f for f in all_subfolders if 'Set_3' in f]\n",
        "trainval_folders = [f for f in all_subfolders if f not in test_folders]\n",
        "\n",
        "def discover_triplets_in_folders(base_dir, folders_list):\n",
        "    folder_items = {}\n",
        "    for sf in folders_list:\n",
        "        sfp = os.path.join(base_dir, sf)\n",
        "        if not os.path.exists(sfp): continue\n",
        "        files = sorted(glob(os.path.join(sfp, '*')))\n",
        "        grouping = {}\n",
        "\n",
        "        # Group BF/Red/Green by filename stem\n",
        "        for f in files:\n",
        "            name = os.path.splitext(os.path.basename(f))[0]\n",
        "            if name[-1].isdigit():\n",
        "                suffix = name[-1]\n",
        "                if name[-2] == '_': key = name[:-2]\n",
        "                else: key = name[:-1]\n",
        "                grouping.setdefault(key, {})[suffix] = f\n",
        "\n",
        "        # Collect valid triplets only\n",
        "        items = []\n",
        "        for k, v in grouping.items():\n",
        "            if '0' in v and '1' in v and '2' in v:\n",
        "                items.append((v['0'], v['1'], v['2'], sf))\n",
        "        folder_items[sf] = sorted(items)\n",
        "    return folder_items\n",
        "\n",
        "folderwise_triplets = discover_triplets_in_folders(BASE, trainval_folders + test_folders)\n",
        "train_items = []\n",
        "val_items = []\n",
        "test_items = []\n",
        "random.seed(42)\n",
        "\n",
        "# Per-folder 80/20 split\n",
        "for folder, items in folderwise_triplets.items():\n",
        "    if folder in test_folders:\n",
        "        test_items.extend(items)\n",
        "    else:\n",
        "        n_total = len(items)\n",
        "        n_train = int(0.8 * n_total)\n",
        "        random.shuffle(items)\n",
        "        train_items.extend(items[:n_train])\n",
        "        val_items.extend(items[n_train:])\n",
        "\n",
        "# Shuffle final lists\n",
        "random.shuffle(train_items)\n",
        "random.shuffle(val_items)\n",
        "random.shuffle(test_items)\n",
        "\n",
        "# Fallback for empty datasets\n",
        "if len(train_items) == 0:\n",
        "    print(\"WARNING: No train data found. Using dummy data.\")\n",
        "    train_items = [(\"dummy_bf.png\", \"dummy_g.png\", \"dummy_r.png\", \"folder\")] * 10\n",
        "if len(test_items) == 0:\n",
        "    test_items = [(\"dummy_bf.png\", \"dummy_g.png\", \"dummy_r.png\", \"folder\")] * 2\n",
        "\n",
        "print(f\"Train: {len(train_items)} | Val: {len(val_items)} | Test: {len(test_items)}\")\n",
        "\n",
        "# === 2) DATASET ===\n",
        "class BFtoFluoDataset(Dataset):\n",
        "    def __init__(self, triplets, out_size=256, augment=False):\n",
        "        self.triplets = triplets\n",
        "        self.out_size = out_size\n",
        "        self.augment = augment\n",
        "        self.is_dummy = \"dummy\" in triplets[0][0] if triplets else False\n",
        "\n",
        "        # Normalize BF and Fluorescence to [-1,1]\n",
        "        self.tf_base = transforms.Compose([\n",
        "            transforms.Resize((out_size, out_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "\n",
        "        # Simple augmentation for BF + target\n",
        "        self.aug_transforms = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.RandomRotation(15)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets) * 2  # Each triplet gives Red + Green\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        triplet_idx = idx // 2\n",
        "        is_red = (idx % 2 == 0)  # Even index = Red, Odd = Green\n",
        "\n",
        "        # Dummy fallback for empty dataset scenario\n",
        "        if self.is_dummy:\n",
        "            return torch.randn(3, 256, 256), torch.randn(1, 256, 256), torch.tensor([1.0, 0.0])\n",
        "\n",
        "        bf_path, green_path, red_path, _ = self.triplets[triplet_idx]\n",
        "\n",
        "        try:\n",
        "            bf = Image.open(bf_path).convert('RGB')\n",
        "            target = Image.open(red_path if is_red else green_path).convert('L')\n",
        "        except Exception:\n",
        "            # Bad/missing image case\n",
        "            return torch.zeros(3, self.out_size, self.out_size), torch.zeros(1, self.out_size, self.out_size), torch.zeros(2)\n",
        "\n",
        "        # Apply identical augmentations\n",
        "        if self.augment:\n",
        "            seed = np.random.randint(0, 99999)\n",
        "            random.seed(seed); torch.manual_seed(seed)\n",
        "            bf = self.aug_transforms(bf)\n",
        "            random.seed(seed); torch.manual_seed(seed)\n",
        "            target = self.aug_transforms(target)\n",
        "\n",
        "        bf_tensor = self.tf_base(bf)\n",
        "        target_tensor = self.tf_base(target)\n",
        "\n",
        "        # One-hot condition: [1,0] red, [0,1] green\n",
        "        cond = torch.tensor([1.0, 0.0]) if is_red else torch.tensor([0.0, 1.0])\n",
        "\n",
        "        return bf_tensor, target_tensor, cond\n",
        "\n",
        "# Create Dataloaders\n",
        "train_dataset = BFtoFluoDataset(train_items, augment=True)\n",
        "val_dataset   = BFtoFluoDataset(val_items)\n",
        "test_dataset  = BFtoFluoDataset(test_items)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=24, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "# === 3) CONDITIONAL DIFFUSION MODEL ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# UNet with attention blocks for better structure preservation\n",
        "unet = UNet2DModel(\n",
        "    sample_size=256,\n",
        "    in_channels=6, # noisy(1) + BF(3) + cond(2)\n",
        "    out_channels=1,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(96, 192, 384, 768),  # Wider network\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "unet.to(device)\n",
        "\n",
        "scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\", prediction_type=\"epsilon\")\n",
        "\n",
        "# AdamW optimizer + Cosine LR schedule\n",
        "optimizer = torch.optim.AdamW(unet.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "T_max = 200 * len(train_loader)\n",
        "lr_scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=1e-6)\n",
        "\n",
        "# Init perceptual loss\n",
        "perceptual_loss_fn = PerceptualLoss().to(device)\n",
        "\n",
        "# === 4) EMA HELPER ===\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.995):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        # Store shadow weights\n",
        "        self.shadow = {name: param.clone().detach() for name, param in model.named_parameters() if param.requires_grad}\n",
        "\n",
        "    def update(self):\n",
        "        # Update running average of weights\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        # Load EMA weights into model\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(self.shadow[name])\n",
        "\n",
        "ema = EMA(unet)\n",
        "\n",
        "# ---\n",
        "## 5. Training Loop\n",
        "print(f\"Starting training on {device}...\")\n",
        "\n",
        "num_epochs = 200\n",
        "best_val_loss = float('inf')\n",
        "history = {'train':[], 'val':[]}\n",
        "\n",
        "L1_WEIGHT = 1.0\n",
        "PERCEPTUAL_WEIGHT = 0.01\n",
        "\n",
        "print(f\"Loss Weights: L1={L1_WEIGHT}, Perceptual={PERCEPTUAL_WEIGHT}\")\n",
        "\n",
        "history_path = os.path.join(OUT_DIR, 'training_history.json')\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    unet.train()\n",
        "    running_train = 0.0\n",
        "    n_train_steps = 0\n",
        "\n",
        "    # === TRAINING ===\n",
        "    for bf, tgt, cond in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
        "        bf, tgt, cond = bf.to(device), tgt.to(device), cond.to(device)\n",
        "\n",
        "        # Sample noise level t\n",
        "        noise = torch.randn_like(tgt)\n",
        "        bs = tgt.shape[0]\n",
        "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bs,), device=device).long()\n",
        "\n",
        "        # Forward diffusion: add DDPM noise\n",
        "        noisy_target = scheduler.add_noise(tgt, noise, timesteps)\n",
        "\n",
        "        # Build condition map\n",
        "        cond_map = cond[:,:,None,None].expand(-1, -1, bf.shape[2], bf.shape[3])\n",
        "\n",
        "        # 6-channel model input\n",
        "        model_input = torch.cat([noisy_target, bf, cond_map], dim=1)\n",
        "\n",
        "        # Predict noise\n",
        "        noise_pred = unet(model_input, timesteps).sample\n",
        "\n",
        "        # L1 noise prediction loss\n",
        "        noise_l1_loss = F.l1_loss(noise_pred, noise)\n",
        "\n",
        "        # Predict x0 for perceptual loss\n",
        "        alpha_t = scheduler.alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "        sqrt_alpha_t = alpha_t.sqrt()\n",
        "        sqrt_one_minus_alpha_t = (1.0 - alpha_t).sqrt()\n",
        "        x0_pred = (noisy_target - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t\n",
        "\n",
        "        perceptual_loss = perceptual_loss_fn(x0_pred, tgt)\n",
        "\n",
        "        total_loss = (L1_WEIGHT * noise_l1_loss) + (PERCEPTUAL_WEIGHT * perceptual_loss)\n",
        "\n",
        "        # Backprop + step\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        ema.update()  # EMA update\n",
        "\n",
        "        running_train += total_loss.item()\n",
        "        n_train_steps += 1\n",
        "\n",
        "    avg_train_loss = running_train / n_train_steps\n",
        "\n",
        "    # === VALIDATION ===\n",
        "    unet.eval()\n",
        "    running_val = 0.0\n",
        "    n_val_steps = 0\n",
        "    with torch.no_grad():\n",
        "        for bf, tgt, cond in val_loader:\n",
        "            bf, tgt, cond = bf.to(device), tgt.to(device), cond.to(device)\n",
        "\n",
        "            noise = torch.randn_like(tgt)\n",
        "            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bf.shape[0],), device=device).long()\n",
        "            noisy_target = scheduler.add_noise(tgt, noise, timesteps)\n",
        "\n",
        "            cond_map = cond[:,:,None,None].expand(-1, -1, bf.shape[2], bf.shape[3])\n",
        "            model_input = torch.cat([noisy_target, bf, cond_map], dim=1)\n",
        "\n",
        "            noise_pred = unet(model_input, timesteps).sample\n",
        "            noise_l1_loss = F.l1_loss(noise_pred, noise)\n",
        "\n",
        "            # Reconstruct x0\n",
        "            alpha_t = scheduler.alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "            sqrt_alpha_t = alpha_t.sqrt()\n",
        "            sqrt_one_minus_alpha_t = (1.0 - alpha_t).sqrt()\n",
        "            x0_pred = (noisy_target - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t\n",
        "\n",
        "            perceptual_loss = perceptual_loss_fn(x0_pred, tgt)\n",
        "            total_loss = (L1_WEIGHT * noise_l1_loss) + (PERCEPTUAL_WEIGHT * perceptual_loss)\n",
        "\n",
        "            running_val += total_loss.item()\n",
        "            n_val_steps += 1\n",
        "\n",
        "    avg_val_loss = running_val / n_val_steps\n",
        "\n",
        "    # Save loss history\n",
        "    history['train'].append(avg_train_loss)\n",
        "    history['val'].append(avg_val_loss)\n",
        "\n",
        "    try:\n",
        "        with open(history_path, 'w') as f:\n",
        "            json.dump(history, f)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ WARNING: Failed to save loss history file. Error: {e}\")\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} | LR: {optimizer.param_groups[0]['lr']:.2e} | Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': unet.state_dict(),\n",
        "            'ema': ema.shadow,\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, os.path.join(OUT_DIR, 'models', 'best_model.pt'))\n",
        "        print(\">>> Saved New Best Model\")\n",
        "\n",
        "# End Training\n",
        "print(\"\\nTraining loop finished.\")\n",
        "print(f\"History saved at: {history_path}\")\n",
        "\n",
        "# ---\n",
        "## 6. Inference\n",
        "print(\"\\n=== Starting Final Inference ===\")\n",
        "\n",
        "def unnorm(x):\n",
        "    \"\"\"Convert [-1,1] back to [0,1]\"\"\"\n",
        "    return (x.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "def inference_loop(bf_img, condition_vec, model, sched, device):\n",
        "    \"\"\"Full DDPM reverse process for one image\"\"\"\n",
        "    img = torch.randn((1, 1, 256, 256)).to(device)  # Start from noise\n",
        "    cond_map = condition_vec[:,:,None,None].expand(-1, -1, 256, 256).to(device)\n",
        "    bf_img = bf_img.to(device)\n",
        "\n",
        "    sched.set_timesteps(1000)\n",
        "    for t in sched.timesteps:\n",
        "        model_input = torch.cat([img, bf_img, cond_map], dim=1)\n",
        "        with torch.no_grad():\n",
        "            noise_pred = model(model_input, t).sample\n",
        "        img = sched.step(noise_pred, t, img).prev_sample  # DDPM update\n",
        "\n",
        "    return img[0]\n",
        "\n",
        "def save_color_panel(bf, red_gt, red_pred, green_gt, green_pred, fname):\n",
        "    \"\"\"Save 5-panel visualization (BF, GTs, predictions)\"\"\"\n",
        "    bf_unnorm = unnorm(bf).permute(1,2,0).cpu().numpy()\n",
        "    r_gt = unnorm(red_gt).permute(1,2,0).cpu().numpy()\n",
        "    r_pred = unnorm(red_pred).permute(1,2,0).cpu().numpy()\n",
        "    g_gt = unnorm(green_gt).permute(1,2,0).cpu().numpy()\n",
        "    g_pred = unnorm(green_pred).permute(1,2,0).cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "    axs[0].imshow(bf_unnorm); axs[0].set_title(\"BF Input\")\n",
        "    axs[1].imshow(r_gt, cmap='Reds_r'); axs[1].set_title(\"Red GT\")\n",
        "    axs[2].imshow(r_pred, cmap='Reds_r'); axs[2].set_title(\"Red Pred\")\n",
        "    axs[3].imshow(g_gt, cmap='Greens_r'); axs[3].set_title(\"Green GT\")\n",
        "    axs[4].imshow(g_pred, cmap='Greens_r'); axs[4].set_title(\"Green Pred\")\n",
        "\n",
        "    for ax in axs: ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fname)\n",
        "    plt.close()\n",
        "\n",
        "# Load best model + EMA weights\n",
        "model_path = os.path.join(OUT_DIR, 'models', 'best_model.pt')\n",
        "SAMPLES_OUT = os.path.join(OUT_DIR, 'samples', 'final_5_panel_color_v3')\n",
        "os.makedirs(SAMPLES_OUT, exist_ok=True)\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    unet.load_state_dict(checkpoint['model'])\n",
        "\n",
        "    class SimpleEMA:\n",
        "        def __init__(self, model):\n",
        "            self.model = model\n",
        "        def apply_shadow(self, shadow_dict):\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if name in shadow_dict:\n",
        "                    param.data.copy_(shadow_dict[name])\n",
        "\n",
        "    ema_loader = SimpleEMA(unet)\n",
        "    ema_loader.apply_shadow(checkpoint['ema'])\n",
        "    unet.eval()\n",
        "    print(\"Loaded EMA model for inference.\")\n",
        "\n",
        "    indices_to_test = range(0, len(test_dataset), 2)\n",
        "    max_samples = 200\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(indices_to_test, total=len(test_dataset)//2, desc=\"Generating Test Samples\"):\n",
        "\n",
        "            bf_r, tgt_r, cond_r = test_dataset[i]\n",
        "            bf_g, tgt_g, cond_g = test_dataset[i+1]\n",
        "\n",
        "            # Generate red + green separately\n",
        "            pred_r = inference_loop(bf_r.unsqueeze(0), cond_r.unsqueeze(0), unet, scheduler, device)\n",
        "            pred_g = inference_loop(bf_g.unsqueeze(0), cond_g.unsqueeze(0), unet, scheduler, device)\n",
        "\n",
        "            out_path = os.path.join(SAMPLES_OUT, f\"final_result_{i//2:04d}.png\")\n",
        "            save_color_panel(bf_r, tgt_r, pred_r, tgt_g, pred_g, out_path)\n",
        "\n",
        "    print(f\"\\nInference complete. Saved to: {SAMPLES_OUT}\")\n",
        "else:\n",
        "    print(\"Model not found — train first.\")\n",
        "\n",
        "print(\"\\nAll done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGkqG4_D6RMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference Only**"
      ],
      "metadata": {
        "id": "3gHiIz_i6f98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inference-Only Script (No GT needed)\n",
        "Generates Red & Green fluorescence from Brightfield images\n",
        "for any images uploaded to /content/\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from diffusers import UNet2DModel, DDPMScheduler\n",
        "\n",
        "# -------------------------\n",
        "# USER INPUTS\n",
        "# -------------------------\n",
        "NEW_FOLDER = \"/content\"  # folder with BF images\n",
        "OUT_DIR = \"/content/output_fluorescence\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/mayank/conditional_diffusion_outputs_v4/models/best_model.pt\"\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# DEVICE\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------\n",
        "# LOAD MODEL\n",
        "# -------------------------\n",
        "unet = UNet2DModel(\n",
        "    sample_size=256,\n",
        "    in_channels=6,\n",
        "    out_channels=1,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(96, 192, 384, 768),\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=1000,\n",
        "    beta_schedule=\"linear\",\n",
        "    prediction_type=\"epsilon\"\n",
        ")\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "unet.load_state_dict(checkpoint['model'])\n",
        "unet.to(device)\n",
        "unet.eval()\n",
        "print(\"Loaded trained UNet model.\")\n",
        "\n",
        "# Apply EMA if present\n",
        "if 'ema' in checkpoint:\n",
        "    class SimpleEMA:\n",
        "        def __init__(self, model):\n",
        "            self.model = model\n",
        "        def apply_shadow(self, shadow_dict):\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if name in shadow_dict:\n",
        "                    param.data.copy_(shadow_dict[name])\n",
        "    ema_loader = SimpleEMA(unet)\n",
        "    ema_loader.apply_shadow(checkpoint['ema'])\n",
        "    print(\"Applied EMA weights.\")\n",
        "\n",
        "# -------------------------\n",
        "# IMAGE TRANSFORMS\n",
        "# -------------------------\n",
        "tf = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])   # BF is RGB\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# UTILS\n",
        "# -------------------------\n",
        "def unnorm(x):\n",
        "    return (x.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "def inference_loop(bf_img, condition_vec, model, sched):\n",
        "    \"\"\"Runs full DDPM reverse pass.\"\"\"\n",
        "    img = torch.randn((1, 1, 256, 256)).to(device)\n",
        "    cond_map = condition_vec[:,:,None,None].expand(-1, -1, 256, 256).to(device)\n",
        "    bf_img = bf_img.to(device)\n",
        "\n",
        "    sched.set_timesteps(1000)\n",
        "    for t in sched.timesteps:\n",
        "        model_input = torch.cat([img, bf_img, cond_map], dim=1)\n",
        "        with torch.no_grad():\n",
        "            noise_pred = model(model_input, t).sample\n",
        "        img = sched.step(noise_pred, t, img).prev_sample\n",
        "    return img[0]\n",
        "\n",
        "# -------------------------\n",
        "# LOAD BF IMAGES\n",
        "# -------------------------\n",
        "def load_bf_images(folder):\n",
        "    IMAGE_EXTS = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\")\n",
        "    bf_files = []\n",
        "    for ext in IMAGE_EXTS:\n",
        "        bf_files.extend(glob(os.path.join(folder, f\"*{ext}\")))\n",
        "    return sorted(bf_files)\n",
        "\n",
        "bf_images = load_bf_images(NEW_FOLDER)\n",
        "print(f\"Found {len(bf_images)} brightfield images.\")\n",
        "\n",
        "# -------------------------\n",
        "# RUN INFERENCE\n",
        "# -------------------------\n",
        "SAVE_DIR = os.path.join(OUT_DIR, \"generated\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "for idx, bf_path in enumerate(tqdm(bf_images, desc=\"Generating Fluorescence\")):\n",
        "    # Load BF image\n",
        "    bf = Image.open(bf_path).convert(\"RGB\")\n",
        "    bf_t = tf(bf).unsqueeze(0).to(device)\n",
        "\n",
        "    # One-hot conditions for Red and Green\n",
        "    cond_r = torch.tensor([[1.0, 0.0]], device=device)\n",
        "    cond_g = torch.tensor([[0.0, 1.0]], device=device)\n",
        "\n",
        "    # Predict Red & Green channels\n",
        "    pred_r = inference_loop(bf_t, cond_r, unet, scheduler)\n",
        "    pred_g = inference_loop(bf_t, cond_g, unet, scheduler)\n",
        "\n",
        "    # Save 3-panel image (BF + Red Pred + Green Pred)\n",
        "    bf_unnorm = unnorm(bf_t[0]).permute(1,2,0).cpu().numpy()\n",
        "    r_pred = unnorm(pred_r).squeeze().cpu().numpy()\n",
        "    g_pred = unnorm(pred_g).squeeze().cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
        "    axs[0].imshow(bf_unnorm); axs[0].set_title(\"BF\")\n",
        "    axs[1].imshow(r_pred, cmap=\"Reds_r\"); axs[1].set_title(\"Red Pred\")\n",
        "    axs[2].imshow(g_pred, cmap=\"Greens_r\"); axs[2].set_title(\"Green Pred\")\n",
        "    for a in axs: a.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, f\"result_{idx:04d}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "print(f\"\\nDONE! All predicted fluorescence saved in:\\n{SAVE_DIR}\")\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "8f779f6c1e6649588e6aafcf9b575c91",
            "84e4aca209fb4a388e3b04d248c2725b",
            "c806c22303814b98b20eeac8703da0e2",
            "9c3f1c48b9ee4bfcb8a7321bc582bf08",
            "62a8596a78ac4504b988efa75df1012d",
            "d9a54173688943be8c91005a8959091e",
            "77b72eab7ce8444a80b1b9d16795e7e2",
            "064b329f1abb4f5880731072bf2439e1",
            "bc234c902a0c40ffa2117ef36c43e598",
            "c20359d92e174958b52103bf75f4f8f9",
            "a0b37a6a66c64c9b8dca00643907bd3b"
          ]
        },
        "id": "0P78IORv6RKe",
        "outputId": "d5138e44-ede5-4946-9cf6-300f0e689dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded trained UNet model.\n",
            "Applied EMA weights.\n",
            "Found 3 brightfield images.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Fluorescence:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f779f6c1e6649588e6aafcf9b575c91"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzoVo6K86wcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}